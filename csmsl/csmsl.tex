\chapter{Software Frameworks for Proteomic Data Analysis}

\section{Summary}
Although the first thing people thing about when talking about proteomics is sample prep, mass spectrometer instrumentation. 
Software and programming tools for data analysis is an important aspect of the proteomic community. In this chapter I describe my work on two open-source software packages used for proteomic analysis. The first is the Coon OMSSA Proteomic Analysis Software Suite (COMPASS)

\section{Introduction}
As the complexity of biological problems grow, so to does the informatic workflows needed to analyze the mounds of data collected. Mass spectrometry is no exception. The mass spectrometers used to collect the data are becoming faster and more sensitive (i.e., more data) with each passing year. Acquiring 20 MS/MS spectra per second is now possible. Additionally, the proteomic analysis is becoming more straightforward and robust, enabling mass spectrometers to be continually ran days and days at a time with very little down time. In the end, hundred of thousands of spectra are collect per instrument in an average day. Keeping up with the data analysis requires sophisticated software and data management tools. These tools must be powerful enough to handle the complexity of the data, flexible to changes and updates in how data is analyzed, and simple enough that non-programmers can efficiently use them. In our lab, we have developed a range of software tools to accomplish these requirements. First, our group has published on a suite of software tools called 'Coon OMSSA (Open Mass Spectrometry Search Algorithm) Proteomic Analysis Software Suite' or COMPASS for short. This package encompasses all the basic tools for analyzing mass spectrometry-based proteomic data. It handles the spectral cleaning and conversion, MS/MS searching (\emph{via} OMSSA), false discovery analysis, protein grouping, various types of quantitation, and post-translational modification localization, among others. To handle the massive number of spectra collected by a host of users per day, we have utilized the High Throughput Condor (HTCondor) system -- developed here on campus, to greatly speed up our database searching program but using hundreds of computers across campus. Lastly, we have developed a open source programming library specifically for proteomic data analysis called C\# Mass Spectrometry Library (CSMSL). This library speeds up the development of software as many common functions and concepts are already fully coded and tested (i.e., peptides, chemical formulas, spectral searching, etc.). This enables even novice programmers to quickly analyze their data in unique fashions without having to reinvent the wheel. Given the complexity of proteomics and mass spectrometry, this allows the user to focus more on the scientific data than the management and construction of complex software. 

\section{COMPASS: Coon OMSSA Proteomic Analysis Software Suite}
The COMPASS program is a complete, standalone data analysis platform for proteomic mass spectrometry. It is based around the Open Mass Spectrometry Search Algorithm (OMSSA) as the primary search engine, but has partially been adapted to handle inputs from other proteomic search engines (i.e., Sequest, Proteome Discoverer). It is written for the Windows operating system using their .NET Framework (v3.5 and above) in the C\# programming language. The main application is an graphical user interface (GUI) with separate windows for each of the programs. Since its first publication in 2010, it has be substantially upgraded to improve the user experience, fix bugs, introduce new features, and for general execution improvements. The source code for COMPASS is available at \url{https://github.com/dbaileychess/Compass} and the current version of it is v1.2.12.8 at publication. The following sections summarize the different parts of the COMPASS program and the improvements from its first publication. 

\subsection*{Database Maker}
Database Maker creates protein databases for target-decoy searching of MS/MS spectra. Text files containing each protein sequence are converted to a decoy version of the same length by reversing, shuffling, or generating random amino acids. The decoy sequences are then concatenated to the input file and exported to another text file. Additionally, the protein sequences are converted to the basic local alignment search tool (BLAST) format for use with OMSSA. Only minor cosmetic changes and bug fixes were made to this program.

\subsection*{DTA Generator}
DTA Generator reduces LC-MS/MS spectral data to merged \texttt{.dta} text files for database searching with OMSSA. Various peak cleaning algorithms are used to simplify spectral prior to searching, these include removal of unreacted precursor, electron-transfer dissociation (ETD) pre-processing to remove precursor, charge-reduced precursors, and neutral losses from charge-reduced precursors. Since the initial publication, additional spectral filters have been added to allow the user more freedom in how the spectra are processed. The outputs generated by the software are also usable by several search algorithms. Although OMSSA is our focus, individual \texttt{.dta} files for SEQUEST or \texttt{.mgf} files for MASCOT are possible outputs. The most significant improvement to DTA Generator since publication was a dramatic decreased in execution time (\textasciitilde50X). This was accomplished by converting the code to utilize multiple processor threads, as well as algorithmic improvements to spectral cleaning. 

\subsection*{OMSSA}
\texttt{OMSSA} is a database search algorithm for proteomic datasets developed at the NCBI by Lewis Geer. It uses a probabilistic scoring to assign confidence that a specific peptide sequence best fits to the experimental spectrum. The program assigns an expectation value (e-value) to each peptide spectrum match (PSM) generated, stating the probability of matching that sequence by random chance. 

\subsection*{HTCondor for OMSSA}
Arguably the biggest improvement to COMPASS since publication is the addition of the High Throughput Condor system for increasing \texttt{OMSSA} searching times. In brief, HTCondor is a computational management system for scheduling processing jobs across a distributed network of computers. Here computers voluntary join a Condor network which enables them to donate their free CPU cycles to other processes to increase the overall bandwidth of the network. This is ideal for large universities, where a majority of networked computers (e.g., computer labs, servers, kiosks, office computers) are not in use 24-7. The Condor system intelligently monitors CPU activity on each attached computer, and given a certain amount of inactivity, reassigns its CPU to process jobs waiting in a queue. If Condor detects new local activity on that computer (e.g., keyboard or mouse movement), it will either pause the Condor job, or transfer it to another inactive computer automatically. Given the large size of the condor network on campus (~7,000 CPUs) there is a high probability that there will always be multiple computers available for analysis. The size of the network makes million of CPU hours available to researches all across campus. From their website, they said ''From July 2011 to June 2012, the [Center for High Throughput Computing] provided 70 Million CPU hours to campus researchers and off-campus collaborators.''

Shortly after COMPASS was published, we developed a GUI program called \texttt{Coondornator} to provide a method for searching MS/MS data \emph{via} OMSSA over the University of Wisconsin's Condor network. The program transfers \texttt{.dta} files (generated by DTA Generator) to the Coon Group fileserver, which is its own 17-CPU HTCondor cluster, for distribution. If more than 17 OMSSA searches are submitted at once, overflow jobs are automatically routed to the Center of High Throughput Computer (CHTC) HTCondor cluster (~6,800 CPUS) for analysis. When the OMSSA searches are completed, the resulting \texttt{.csv} file containing all the PSMs are transfered back to the Coon Group fileserver. This program provides a seamless integration between the HTCondor network and end user's computer, making high throughput computing no harder than running a program on a computer. The Coon Group routinely searches hundreds of \texttt{.dta} containing million of spectra on the HTCondor network. This represents a significant throughput gain compared to searching files on individual desktop computers. Conservative estimated of the time savings is between 30-50X on average. For example, to search all the MS/MS spectra from a one hour LC-MS/MS experiment of a tryptic digestion of whole-cell yeast cells would take about 30-40 minutes on a normal desktop computer. In contrast, if the all the MS/MS spectra were split into groups of 1000 spectra each, and searched using \texttt{Coondornator} over the distributed network, the same results could be generated in \textasciitilde1 minute, a 30X decrease in execution time.

\subsection*{FDR Optimizer}
\texttt{FDR Optimizer} statistically filters PSMs generated from \texttt{OMSSA} to control for false identification. The program maximizes the number of true positive identifications at a given error rate, typically set to under 1\%. \texttt{FDR Optimizer} can work on either low-resolution MS1 data with a simple e-value filter, or on high-resolution MS1 data with a two dimensional filter on precursor mass error and e-value. To use \texttt{FDR Optimizer}, both target and decoy peptide sequences have to be searched with \texttt{OMSSA} on the same set of spectra for adequate false discovery filtering. 

For low-resolution datasets, PSMs are first loaded into the program and the best scoring PSM (i.e., lowest e-value) for each spectrum is saved and all other PSMs are discarded. The remaining PSMs are then sorted on their e-value, from smallest to largest. Each PSM has a field to indicate whether it results from a target protein or a decoy protein. A counter for both the number of targets ($T$) and decoy ($D$) peptides identified is kept as the program iterates over the sorted PSMs. When the false discovery rate (FDR, Equation \ref{eq:fdr}) increases over some specified value (e.g., 1\%), the program stops.
\begin{equation}
FDR =\frac{D}{T + D}
\label{eq:fdr}
\end{equation}
The PSMs that have been already been processed are saved to a \texttt{.csv} file and represents the true identifications. 

High-resolution datasets can be processed with an additional filter to increase the number of identifications from a dataset. First each PSM is read into the program and its precursor mass error is determined from the MS1 spectrum. The median precursor mass error of all PSMs is then computed and each PSM is corrected by this median value. This corrects any systematic mass error the mass spectrometry had, and usually reduces the mass errors to under 5 ppm. \texttt{FDR Optimizer} then iteratively sets a maximal ppm error allowed, and filters the PSMs to contain only PSM with a precursor mass error lower than the maximum. These filtered PSMs are then processed identically to the low-resolution dataset described above. This whole process is then repeated with a slightly larger maximal ppm error, and the number of identifications is recorded. The program tries all possible maximal ppm errors and reports the ppm error with the most true identifications at the end. This maximizing algorithm increases the number of true identifications produced then the simple low-resolution filter.

Since publication, \texttt{FDR Optimizer} has been completely rewritten. Previously, four separate programs were used and maintained: \texttt{Low-Resolution FDR Optimizer}, \texttt{FDR Optimizer}, \texttt{Batch Low-resolution FDR Optimizer}, and \texttt{Batch FDR Optimizer}. The current version simplifies the user experience by combining all four programs into one, with simple option check-boxes to indicate the desired analysis. Improvements to the FDR analysis and maximizing algorithms have also lead to a large decrease in execution time (\textasciitilde10-20X).

\subsection*{TagQuant}
\texttt{TaqQuant} extracts and processes isobaric labeling quantitative information from MS/MS spectra. It is compatible with both the major types of isobaric labels, TMT and iTRAQ. \texttt{TagQuant} obtains intensities of the reporter ions of interest from the raw data. These intensity values are subsequently denormalized by multiplying by the ion injection time to yield the number of ion counts detected, a quantity which can be fairly compared across different spectra and analyses. Purity correction is then applied, as has been previously published,\cite{itracker} using user-specified purity data provided by the manufacturer. Finally, normalization is performed such that the total intensity of each tag is equal, accounting for differences in sample mixing quantities.

Numerous improvements have been made to the publication version of \texttt{TaqQuant}. With the advent of high-resolution TMT tags, where two quantiation channels are separated by a very small mass difference (6.32 mDa), additional logic had to be added to handle it. Users also started to mix and match channels between different manufacturing lots, resulting in non-standard purity values. This, and other issues, were corrected by providing the user full control over which labels then use to quantify and their respective purities. This also future proofs the program, allowing it to handle any type of isobaric label later developed.

\subsection*{Protein Hoarder}
\texttt{Protein Hoarder} infers the most likely proteins identified based on the peptides validated by \texttt{FDR Optimizer}. The program was initially called \texttt{Protein Herder}, but the program was completely rewritten after publication. 
The peptides are assembled into protein groups based on the law of parsimony, i.e., the minimum number of protein groups to account for all the identified peptides. The biggest change between the original and current version is how the peptides are found within the candidate proteins. Previously, each peptide sequence was brute force searched against the whole protein database. For large databases such as the Human proteome, the number of proteins could reach over 50,000 when both target and decoy proteins are considered. This made the original program very slow, and could take up to half a day to assembly protein groups for a human sample. The new algorithm forgoes the string search and uses enzymatic cleavage of the proteins to find the peptides. The program preforms an \emph{in silico} digestion of all the proteins and if a generated peptide matches one of the input PSMs, that protein is saved. This process greatly speeds up the whole program, and the same human sample that took half a day to assembly takes less then 2 minutes. 

Assembled protein groups are further filtered for false discovery using a similar method to \texttt{FDR Optimizer}. Here, the p-value of the protein group (which is the product of the group's peptide's e-values) is the ordering metric and the groups are filtered to a specified FDR (e.g., 1\%). In the publication version of COMPASS, there was another program that handled protein quantitation (\texttt{Protein TagQuant}) by summing up the peptide quantitation for a individual protein group. This program was embedded into \texttt{Protein Hoarder} since all the required information for quantitation was already present in the program. Here, peptides that are not shared between protein groups (i.e., a unique peptide to the protein group) have their quantitation summed and reported for the protein group.

\subsection*{LoToR}
\texttt{LoToR} (\underline{Lo}calize \underline{To} \underline{R}esidue) improves the localization of post translational modifications to specific residues on peptides and proteins. Although \texttt{OMSSA} is capable of identifying modification events on peptides, it often does not place the PTM on the correct residue. To address this, \texttt{LoToR} was created to supplement this shortfall and add more rigorous statistical power in localizing PTMs. 

\texttt{LoToR} is uses the A-Score algorithm as the primarily metric for assigning statistical confidence.\cite{ascore} In brief, for each PSM that contains a PTM, all possible peptide isoforms are generated and are matched to the MS/MS spectrum. The set of fragment ions that can distinguish two isoforms apart from each other are called 'site-determining fragments' (SDFs). The number of identified SDFs for each isoform is then pair-wised compared, and the two isoforms that have the biggest difference between number of SDFs identified, is saved.

\section{CSMSL: C\# Mass Spectrometry Library}
Mass spectrometry-based proteomics is a young field that is rapidly evolving, new techniques and technologies are consistently being developed and custom software tools are needed to analyze the data. There are typically three ways to analyze a proteomic dataset: 1) process it through a full-fledge GUI program that has already been developed, 2) manually process the data, or 3) extract data with software tools and analyze in excel. Often a mixture of these three processes are needed to fully characterized a dataset. However, sometimes a complete GUI program is not available for a specific type of data analysis, or, due to the complexity of the data, manual analysis of a dataset in excel or through a spectrum browser (e.g., Thermo XCalibur) is a daunting and time-consuming task. These situations are ideal for a custom analysis program that could facilitate the analysis. Unfortunately, custom programs have shortcomings, 1) not every single researcher knows how to program them and 2) there isn't a free, straightforward programming environment for accessing and manipulating such complex data. The first problem is not easily addressed, but the second one is. There are many tools available for MS analysis on the internet, but most are difficult for novice programmers and are challenging to adapt to a specific need. To fill the gap, I have designed a large proteomic programming library to simplify the data management and manipulation of large-scale proteomic data. It is written for Windows using the .NET Framework V4.0 in the C\# programming language. It is called C\# Mass Spectrometry Library (\texttt{CSMSL}) and is freely available at \url{https://github.com/dbaileychess/CSMSL}.

The goals of \texttt{CSMSL} is to provide an easy-to-use, powerful, feature-rich library of .NET C\# objects and methods to enable even novice programmers the ability to analyze proteomic data quickly. Simplicity is key, calculating the mass of the peptide sequence 'CSMSL' only requires the following two lines:

\lstset{style=sharpc}
\begin{lstlisting}
Peptide peptide = new Peptide("CSMSL");
Console.WriteLine(peptide.Mass.Monoisotopic);
// outputs : 539.20835516707
\end{lstlisting}

In addition to simply syntax, CSMSL is designed with performance in mind, allowing even computationally intensive calculations to be completed quickly. For example, a complete yeast database (6,627 proteins) can be loaded from a \texttt{.fasta} file, digested with trypsin (up to 3 missed cleavages, 5 to 35 amino acids in length) in under 2 seconds. If we include the calculation for the +1 \mz{} of each of the 913,740 resulting peptides, the total time only goes up to 4 seconds (this includes full chemical formula determination). While \texttt{CSMSL} is not expected to meet the performance of advanced compiled languages (e.g., C/C++, Fortran, etc.), its adequate performance plus simplicity of use are sure to be helpful in analyzing data in new and creative ways without significant overhead.

The following sections will succinctly 1) describe the design of the library, 2) show a few example code segments indicating its use, and 3) highlight various features.

\subsection*{Design}
The \texttt{CSMSL} package is divided into three projects. The main project is the library itself (\texttt{CSMSL.csproj}) which contains all the code and objects for analysis. This project will be described in greater detail in the sections to follow. The other two projects are primary for teaching and development purposes, and will be described here. 

The teaching project is \texttt{CSMSL.Examples.csproj}, which contains short segments of code to show intended use of the library. It is written to aid novice programmers in learning how to program better and how to use the library. The examples cover how to create peptides and proteins, digest proteins, fragment peptides, read in data, among many others. This project is completely standalone and is only used to demonstrate the main library. 

The development project is \texttt{CSMSL.Tests.csproj}, which hosts all the unit tests for the library. An unit test is a short piece of code that tests one, and only one aspect of the library, hence the term 'unit'. In brief, a short segment of code is written to preform some action (e.g., digest a protein), and the final line of code segment is an assertion statement, declaring some value to possess some trait (e.g., 5 peptides are produced from a digestion). These assertions can be as simple as an equality ($numOfPeptides == 5;$), a comparative condition ($numOfPeptides < 5;$), or much more advanced comparisons. Regardless, the point of unit tests is to provide fine grain support and testing for the main project. If any source code is added to the project, all the unit tests report back if their one piece of functionality is still producing the same result. This helps ensures that new features do not affect other parts of the library or produce unintended bugs. \texttt{CSMSL} is heavily unit tested on the most commonly used parts.

There also exists a handful of other projects that supply support for third-party tools and access to raw spectral data from different MS vendors. These are located under the \texttt{CSMSL/IO} directory and can be added to a project when needed.

\subsection*{Examples}
Most coding examples are contained with the \texttt{CSMSL.Examples} project, and is useful for newcomers. Below are a series of examples showing the simplicity and power \texttt{CSMSL} offers. All of the examples are in the C\# language and should be straightforward enough that even non-programmers should be able to follow them.

We will start with the most basic, but most commonly used, feature: Proteins and peptides. The following code constructs a peptide object, labeled \texttt{peptideA} and then prints its monoisotopic mass to a window.

\begin{lstlisting}
Peptide peptideA = new Peptide("FLTTSNALKEN");
Console.Write(peptideA.MonoisotopicMass);
// outputs: 1236.635016661
\end{lstlisting}

Of course there are a plethora of tools and websites that could calculate the monoisotopic mass of a peptide sequence, the novel aspect is the simplicity and the ability to programmatically control the action. Peptides and proteins can be modified post transitionally and \texttt{CSMSL} enables easy methods for modifying peptides. Taking the previous example further, to modified both 'S' residue with a phosphorylation is as easy as follows.

\begin{lstlisting}
Peptide peptideA = new Peptide("FLTTSNALKEN");
ChemicalFormula phospho = new ChemicalFormula("HPO3");
peptideA.SetModification(phospho, 'S');
Console.Write(peptideA.MonoisotopicMass);
// outputs: 1316.60134718175
\end{lstlisting}

This examples introduces a new concept of a \texttt{ChemicalFormula}, which represents a chemical structure, in this case a phopshoryl group. The third line sets this chemical formula to be the modification on the single 'S' in the sequence. The resulting mass is $1316.601347$ and the difference is $79.96637$ which corresponds to the mass of an phosphorylation.

Peptides arise from the enzymatic digestion of intact proteins by proteolysis from a protease, such as trypsin. \texttt{CSMSL} can do the same thing done in a test tube \emph{in silico}. Below is an example of a tryptic digestion of a single protein. It produces a list of Peptide objects which are printed to the screen.

\begin{lstlisting}
Protein proteinA = new Protein("MMGFKQLITTGSSSRSSSSKDTSST");
List<Peptide> peptides = proteinA.Digest(Protease.Trypsin);
Console.Write(peptides);
// outputs: MMGFK, QLITTGSSS, SSSSK, etc...
\end{lstlisting}

There exists many options for digestions, such as maximum and minimum peptide length, max missed cleavages, partial digestion, etc. All these options will not be explored here, but almost anything you could do on a physical protein/peptide, can be performed \emph{in silico} using \texttt{CSMSL}.

\subsection*{Features}
The \texttt{CSMSL} library has too many features to be listed in full, so only a few of the most important features will be highlighted here. Since this is primarily a proteomic library, we will start off with proteins and chemicals and then transition to spectral classes. 

Starting from the smallest object and growing bigger, elemental isotopes represent the basic building block of everything else that has mass. Each isotope has a few intrinsic properties, most important are mass and the element it belongs to. A single element may contain a set of different isotopes (e.g., $^{12}C$, $^{13}C$, and $^{14}C$), and the most naturally-abundant isotope is declared the principal isotope of the element. Thus elements and their most abundant isotopes are interchangeable with each other (i.e., $^{12}C$ and $C$ refer to the same object). When another isotopes is needed, you need to specify which isotope you want to use (e.g., $C\{13\}$ mean you want $^{13}C$ instead of $^{12}C$). This feature is important because stable isotope quantitative labeling is very common analysis, and we wanted to design the library with it in mind. All the elements, and thus isotopes, are assembled into the periodic table of elements for easy access.

In \texttt{CSMSL}, chemical formulas are represented as a set of isotopes without any spatial connectivity. Keeping the three dimensional structure of molecules is not an important aspect of most proteomic work, and we purposefully left this out in favor of speed and memory savings. The mass of a chemical formula is the simple summation of all of its isotopes. Almost every other object in this library is a chemical formula (e.g., proteins, peptides, amino acids, modifications, etc.)  with additional properties of its own. Amino acids are simply a chemical formula with character symbol to represent which one it is. The 20 common amino acids are prebuilt by the library and ready to use, but custom amino acids can be added easily.

Probably the most important classes in the library are the protein and peptide classes. Since both a protein and peptide can be thought of as a string of amino acids, both classes are modeled off a single base class called \texttt{AminoAcidPolymer}. This class can just be thought of as an fancy array of amino acids, spanning from the N-terminus to the C-terminus. Each location on this array (i.e., amino acids or termini) can be modified by a chemical formula. The mass of the \texttt{AminoAcidPolymer} is again the simple summation of all its amino acids and modifications. Peptides have special methods for producing fragments ions (e.g., a, b, c, x, y, z-type), as well as others. Fragment ions again are just chemical formulas, but they keep track of what amino acids and modifications are contain in each fragment. This is particularly useful when matching fragment ions against a mass spectrum, as it fully annotates the spectra during the matching steps. Proteins have their own special methods for digestion. 



\subsection*{Spectral Data Access}
Perhaps the most useful feature \texttt{CSMSL} provides is access to raw spectral data collected by the MS. Instrument vendors usually offer an application programming interface (API) for accessing data from their propriety data formats (e.g., \texttt{.raw} for Thermo, \texttt{.d} for Agilent, etc.). While it is possible to use them on own, a few factors make them difficult to implement. First, they are geared to more advanced programmers and often have incomplete documentation. This makes learning how to access the data difficult, even for good programmers. For people who don't know how to program at all, it would be very difficult to understand and make work. Secondly, each instrument vendor creates their own API which is incompatible with everyone else. Thus if you desire to analyze two different types of data with your program, you'll have to use both APIs to achieve the same result. This can often lead to bugs and frustration. Lastly, you have to be an expert in each API in order to fully use their capabilities. \texttt{CSMSL} solves these issues by having a single and simple interface to accessing the data. No matter where the data was produced, the same code can access the underlying data, without the users having to fiddle around. 

The following example shows how to read in every MS spectra from a \texttt{.raw} file from a Thermo mass spectrometer.
\begin{lstlisting}
MSDataFile dataFile = new ThermoRawFile("somerawfile.raw")
dataFile.Open();                   
foreach (MSDataScan scan in dataFile)
{             
    Console.WriteLine('Number of Peaks: '+scan.PeakCount);
}
\\ outputs:
\\   Number of Peaks: 1052
\\   Number of Peaks: 523
\\   etc..
\end{lstlisting}
First, a mass spectrum data file is constructed from a file on the computer, named \texttt{somerawfile.raw}. The second line opens a connection to the file, and the third line iterates over each MS scan within that file. The number of peaks contained within that scan is then printed to the screen. The beauty of this is that the first line could be changed to:
\begin{lstlisting}
MSDataFile dataFile = new AgilentDDirectory("somerawfile.d")
\end{lstlisting}
and the program would continue to work. While there are too many features to be fully explained here, the concept of a simple and consistent way to access spectral data is a key component of \texttt{CSMSL}. 