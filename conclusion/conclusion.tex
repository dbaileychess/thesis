\chapter{The Future of Intelligent Data Acquisition Methods}

\section{Summary}
The work presented here describes technologies on the forefront of mass spectrometry acquisition methods. These methods utilize data analysis algorithms--typically perform post-acquisition, during the acquisition of MS spectra to improve data quality. The ability to immediately analyze spectra and then make informed decisions on how to proceed is important in separation-based analyses, where an analyte is only accessible for a short time period. This document has looked at the history of MS acquisition methods and started with a discussion on data-dependent acquisition and other acquisition methods. The second chapter described our work on intelligent data acquisition (IDA) methods; we developed the first online spectral database search (\inseq{}), to improve multiple aspects of data acquisition. The following chapter continued on this theme and focused on improving the reproducibility of peptide identification by using real-time elution ordering scheduling. The fourth chapter took a behind-the-scenes look on the programming environments used and developed to enable IDA methods. In this chapter, various challenges to intelligent data acquisition--both scientific and practical, are discussed and possible solutions are proposed.

\section{Introduction}
Data-acquisition methods are an important part of mass spectrometry analysis. This is especially true when the MS is coupled to a separation technique such as liquid or gas chromatography (LC-, GC-MS). Here a complex sample is separated over time and only a subset is analyzed at any point in time. In LC-MS, analytes may only elute for 15-30 seconds in total, and in GC-MS times are even shorter (5-10 seconds). The MS has a limited amount of time to detect them. For proteomic work, to identify the analytes (e.g., peptides and proteins) a MS/MS spectra must be collected to determine its amino acid sequence. Thus it is not only important to detect the presence of the precusor in a MS1 scan, it also has to be isolated, fragmented and mass analyzed again in a MS/MS to determine its identity, all of which takes time. Given the complexity of proteomic samples (thousands of proteins digested into hundreds of thousands of peptides) the main challenge becomes one of time-management, i.e., how should the mass spectrometer spend its time?

The most straightforward answer is to speed things up, make the mass spectrometer faster and more sensitive so it can spend less time per analyte, and therefore gains access to more of the proteome. The fastest mass spectrometers can achieve nearly 20 Hz scan rates while still being sensitive and selective enough to identify peptides. But increased speed can only solve so much of the time-management problem. Take yeast for example, with approximately 6,600 proteins that produce half a million peptides when digested with trypsin (1 missed cleavage), how long would it take to sample each peptide? Assuming a 20 Hz acquisition rate, an 100\% identification rate, and a perfect LC separation, it would take at least 7 hours of constitutive operation to identify each once. Of course identifying every peptide in a solution isn't required to learn about the proteins in the sample, this just illustrates that speed alone will not immediately solve the challenge. The other factors, like perfect separations and high identification rates, represent greater challenges to achieve.

Another approach in solving the time-management challenge is to better allocate the mass spectrometer resources to identify the ''most useful'' parts of the samples. Effective allocation requires information, and our approach provides the mass spectrometer with more options and information through software modifications and real-time data analysis. Here, the mass spectrometer can gain information about the sample in an automatic and dynamic fashion, and can change course when it sees fit. Software improvements are ideal, since they cost nothing to deploy and can modify existing instruments without hardware upgrades. However, since intelligent methods are still in their infancy, much work needs to be done. The following sections outline the two largest hurdles that need to be cleared in order that IDA methods are widely used and useful. First, there must be improvements made to how the analysis is conducted and how to respond the results. The surface has barely been scratched, and substantial improvements over other techniques need to be demonstrated before more researchers will use them. The other challenge for IDA methods is that they lack general accessibility. It is difficult to implement such methods on your own, and instrument vendors have been reluctant to distribute and allow access to developed methods. Changes in the way instruments are controlled and management is a key issue in the future of intelligent data acquisition methods.

\section{Improving Decision Making in Intelligent Data Acquisition}

To achieve wide-spread use of IDA requires proving and improving its usefulness to the other researchers. If some other method can accomplish the same task, or do it better, then IDA methods will not be used. We have demonstrated that IDA methods can improve certain aspects of data collection. In chapter 2, several types of improvements made by IDA were outlined, such as improved quantitation and in chapter 3 we increased run-to-run reproducibility. We believe that IDA methods are capable of improving data quality and throughput. However, additional work needs to be done to 1) make IDA methods even better than traditional analysis 2 )improve the analysis algorithms and 3) make smarter real-time decisions.

To allocate resources and maximize results, the mass spectrometer needs the best available information in the shortest amount of time. This involves designing algorithms to analysis spectra quickly and accurately. Any delays caused by data-analysis further hinder IDA methods compared to more traditional analyses. Much of the work in this document was developed with the ion-trap control language (ITCL) which is a synchronous language. This means that only one thing can be done at a time, and would effectively block the instrument from scanning while analyzing the previous spectrum. Developing a system where the instrument performance is not negatively affected by the real-time data analysis is very important step to improve the results.

Improvements must also be made to the decision making steps that follow. There is no benefit in analyzing a MS spectrum in real-time if there is no response to the results. Appropriate responsive action is necessary in improving data quality. Deciding what to do and when to do it becomes one of the biggest challenges to IDA methods. For example, in a middle of a LC-MS/MS experiment, \inseq{} identified a peptide with a post-transitional modification from a MS/MS spectra. However, the spectral quality is not good enough to provide specific localization of the PTM. What should the mass spectrometer do next? Resample it with a different dissociation technique? Increase the resolution? Finding the answer to this and other possible scenarios is an important part of IDA and needs to be more fully explored. Actions may be dependent on the sample, or the type of analysis being perform, and may change from experiment to experiment. 

\section{Accessibility of Intelligent Mass Spectrometers}
The other issue that faces intelligent data acquisition methods is the lack of general availability to researchers. Enabling new methods requires modification of the instrument control logic, which is not always straightforward to implement. There are two ways for increasing the intelligence of mass spectrometers. The first would be to modify a home built mass spectrometer, where the researcher has full control over the control logic. The other way is to modify and extend commercially available mass spectrometers with the desired abilities. For large-scale proteomic work, the former approach is not straightforward, as a vast majority of publications use commercial instruments for data acquisition. Custom built mass spectrometers often focus on a very specific task (e.g., mass analyzer development, new dissociation techniques, etc.) and rarely are geared for high-performance, large-scale LC-MS protein experiments. Even if a researcher built a mass spectrometer capable of these types of experiments, there is no easy way to disseminate the technology, short of starting a company themselves and selling their work. On the other hand, commercial instruments are primarily developed to take the best technologies available and combine them into one unified package. This results in a powerful and stable instrument that can handle the largest experiments. However, in order to protect their intellectual property (IP), instrument vendors are usually highly restrictive in how their instruments are used and modified. This makes implementing novel acquisition methods very difficult, and therefore general acceptance of new methods is slow. Thus, increasing the accessibility and availability of IDA methods is the a very important factor in its future use.

Probably the best way to propel the development of intelligent acquisition methods forward is to increase its accessibility and availability to researchers. This is challenging since instrument vendors are highly protective of their products; they have to protect their intellectual property and public imagine while providing state-of-the-art technologies to consumers. They are wary of providing access to their control logic for fear of competition. They also worry that supporting third-party programs for their instruments could damage their reputation. Our lab, which has developed multiple technologies now commercialized, knows first hand the care instrument vendors take in releasing third-party technologies to the general consumer. The following section will briefly discuss how developing new technologies is currently done and improvements that could be made to facilitate the dissemination of intelligent mass spectrometers.  

\subsection*{Instrument Programming.}
To develop new MS instrument methods, researchers are typically given special access to the instrument's firmware by a vendor. This allows them direct control of the instrument and gives them the ability to alter the methods as they see fit. However, this method of development is not ideal and is more of band-aid fix then a real solution. Most instrument vendors never developed a system to support third-party methods, so when the first researchers wanted to extend the instrument capabilities, the quickest solution was to give them access to the source code, just like they were an internal developer. After they have developed their technologies, in order for other research groups to use them, the developers had to work with the vendors to commercialize their products. This process is ineffective and slow, and prohibits mass distribution. 

A better model of development would be to develop and deploy an application programming interface (API) to control the instrument. Here, the vendor would provide a set of software tools and objects to enable access to different parts of the instrument. They would have fine-grain support on what they make available to the end user and what they keep hidden, thus alleviating some IP concerns. This technique also could provide error checking, preventing the user from setting some value that could potentially damage the instrument or injure someone. In fact, some instrument vendors have started down this path. Thermo has recently released an API for their Q Exactive mass spectrometer to enable third-party support. This allows the user to program in an more advanced language then is used on the instrument itself. For example, the Q Exactive's firmware is written in Python (a scripting language), while the API is written in C\# (a compiled, and generally a more powerful language). This makes programming the instrument compatible with libraries such as CSMSL discussed in Chapter 4. An API model also allows users to share their code without IP issues, and could greatly improve the code when multiple developers are working together. 

The biggest challenge is convincing the instrument vendors to support such a technology, as it requires time and resources to develop and maintain. But if anything can be learned from community-developed applications on the internet (i.e., crowd sourcing), much can be gained when many people are working on a common problem. It may well behoove vendors to provide such access to potentially gain dozens of developers. The other issue is the distribution of software to other researchers. An ideal solution would again mimic the internet, by constructing a central marketplace to download and install methods. This would greatly facilitate the distribution aspect and improve the accessibility.

