\chapter{The Future of Intelligent Data Acquisition Methods}

\section{Summary}
Intelligent data acquisition methods are technologies on the forefront of mass spectrometry. These methods utilize data analysis algorithms---typically perform post-acquisition, during the acquisition of MS spectra to improve data quality. The ability to immediately analyze spectra and then make informed decisions on how to proceed is important in separation-based analyses, where an analyte is only accessible for a short time period. This document has looked at the history of MS acquisition methods and started with a discussion on data-dependent acquisition and other acquisition methods. The second chapter described our work on intelligent data acquisition (IDA) methods; we developed the first online spectral database search (\inseq{}), to improve multiple aspects of data acquisition. The following chapter continued on this theme and focused on improving the reproducibility of peptide identification by using real-time elution ordering scheduling. The fourth chapter took a behind-the-scenes look on the programming environments used and developed to enable IDA methods. In this chapter, various challenges that confront intelligent data acquisition---both scientific and practical, are discussed and possible solutions are proposed.

\section{Introduction}
Data-acquisition methods are an important part of mass spectrometry analysis. This is especially true when the MS is coupled to a separation technique such as liquid or gas chromatography (LC-, GC-MS). Here a complex sample is separated over time and only a small subset is analyzed at any point in time. In LC-MS, analytes may only elute for 15-30 seconds and in GC-MS times are even shorter (5-10 seconds), in either case the MS has a limited amount of time to detect them. For proteomic work, to identify the analytes (e.g., peptides and proteins) a MS/MS spectra must also be collected to determine the amino acid sequence. Thus it is not only important to detect the precusor in a MS scan, it also has to be isolated, fragmented and mass analyzed again (MS/MS) to determine its identity, all of which takes time. Given the complexity of proteomic samples (thousands of proteins digested into hundreds of thousands of peptides) the main challenge becomes one of time-management, i.e., how should the mass spectrometer spend its time?

The most straightforward answer is to speed things up, make the mass spectrometer faster and more sensitive so it can spend less time per analyte, and therefore gains access to more of the proteome. The fastest mass spectrometers can achieve nearly 20 Hz scan rates while still being sensitive and selective enough to identify peptides. But increased speed can only solve so much of the time-management problem. Take yeast for example, with approximately 6,600 proteins that produce half a million peptides when digested with trypsin (1 missed cleavage), how long would it take to sample each peptide? Assuming a 20 Hz acquisition rate, an 100\% identification rate, and a perfect LC separation, it would take at least 7 hours of constitutive operation to identify each once. Of course identifying every peptide in a solution isn't required to learn about the proteins in the sample, this just illustrates that speed alone will not immediately solve the challenge. The other factors, like perfect separations and high identification rates, represent greater challenges to solve.

Another approach in solving the time-management challenge is to better allocate the mass spectrometer resources to identify the ''most useful'' parts of the samples. Effective allocation requires information, and our approach provides the mass spectrometer with more options and information through software modifications and real-time data analysis. Here, the mass spectrometer can gain information about the sample in an automatic and dynamic fashion, and can change course when it sees fit. Software improvements are ideal, since they cost nothing to deploy and can modify existing instruments without hardware upgrades. However, since intelligent methods are still in their infancy, much work needs to be done. The following sections outline the two largest hurdles that need to be cleared in order that IDA methods are widely used. First, there must be improvements made to how the analysis is conducted and how to respond to the results. This surface has barely been scratched. IDA methods need to demonstrate substantial improvements over other techniques before more researchers will use them. The other challenge for IDA methods is that they lack general accessibility. It is difficult to implement such methods on your own, and instrument vendors have been reluctant to distribute them. Changes need to be made on how instrument vendors provide access to new methods before they see wider use.

\section{Improving Decision Making in Intelligent Data Acquisition}

Increasing the use of IDA requires proving and improving its usefulness to the other researchers. If some other method can accomplish the same task, or do it better, then IDA methods will not be used. We have demonstrated that IDA methods can improve certain aspects of data collection. In chapter 2, several types of improvements made by IDA were outlined, such as improved quantitation for isobaric labels and SILAC, as well better PTM localization. Chapter 3 discussed increases in run-to-run reproducibility of peptide identification. We believe that IDA methods are capable of improving data quality and throughput in other areas as well. However, additional work needs to be done to 1) make IDA methods even better than traditional methods (e.g., DDA), 2) improve the algorithms used to analyze spectra and 3) make smarter real-time decisions.

To allocate resources efficiently and maximize data quality, the mass spectrometer needs the best available information in the shortest amount of time. This involves designing algorithms to analysis spectra quicker and more accurately. Any delays caused by data-analysis further hinders IDA methods compared to more traditional methods---which due to their simple construction take minimal time to execute. Unfortunately, the methods described in this document were developed using the ion-trap control language (ITCL) which lacks many features. One missing feature that greatly hinders IDA methods is the lack of asynchrony---only one thing can be done at a time. The MS could not set variables for the next scan while analyzing the previous spectrum, it would have to wait till the analysis step was complete. Developing a system where the instrument performance is not negatively affected by the real-time data analysis is very important step to improve the results. 

Improvements must also be made to the decision making steps that follow real-time analysis. There is no benefit in analyzing a MS spectrum in real-time if there is no response to the results. Appropriate responsive action is necessary in improving data quality. Deciding what to do and when to do it becomes one of the biggest challenges to IDA methods. For example, in a middle of a LC-MS/MS experiment, \inseq{} identified a peptide with a post-transitional modification from a MS/MS spectra. However, the spectral quality is not good enough to provide specific localization of the PTM. What should the mass spectrometer do next? Resample it with a different dissociation technique? Increase the resolution? Finding the answer to this and other possible scenarios is an important part of IDA and needs to be more fully explored. The work described in this document only briefly explored possible actions and a lot more work can be devoted to increasing this aspect of the IDA methods. These actions also may be dependent on the sample, or the type of analysis being perform, and may change from experiment to experiment. So providing a robust set of options that can cover a multitude of experimental conditions is challenging.

\section{Accessibility of Intelligent Mass Spectrometers}
The other issue that faces intelligent data acquisition methods is the lack of general availability to researchers. Enabling new methods requires modification of the instrument control logic, which is not always straightforward to implement. There are two ways for increasing the intelligence of mass spectrometers. The first would be to modify a home built mass spectrometer, where the researcher has full access over the control logic. The other way is to modify and extend commercially available mass spectrometers with the desired abilities. For large-scale proteomic work, the former approach is not straightforward, as a vast majority of publications use commercial instruments for data acquisition. Custom built mass spectrometers often focus on a very specific task (e.g., mass analyzer development, new dissociation techniques, etc.) and rarely are geared for high-performance, large-scale LC-MS protein experiments. Even if a researcher built a mass spectrometer capable of these types of experiments, there is no easy way to disseminate the technology, short of starting a company themselves and selling their work. On the other hand, commercial instruments are primarily developed to take the best technologies available and combine them into one unified package. This results in a powerful and stable instrument that can handle the largest experiments. However, in order to protect their intellectual property (IP), instrument vendors are usually highly restrictive in how their instruments are used and modified. This makes implementing novel acquisition methods very difficult, and therefore general acceptance of these methods is slow. Thus, increasing the accessibility and availability of IDA methods is the a very important factor in its future use.

Probably the best way to propel the development of intelligent acquisition methods forward is to increase its accessibility and availability to researchers. This is challenging since instrument vendors are highly protective of their products; they have to protect their intellectual property and public imagine while providing state-of-the-art technologies to consumers. They are wary of providing access to their control logic for fear of competition. They also worry that supporting third-party programs for their instruments could damage their reputation if things go wrong. Our lab, which has developed multiple technologies now commercialized, knows first hand the care instrument vendors take in releasing third-party technologies to the general consumer. The following section will briefly discuss how developing new technologies is currently done and improvements that could be made to facilitate the dissemination of intelligent mass spectrometers.  

\subsection*{Instrument Programming.}
To develop new MS instrument methods, researchers are typically given special access to the instrument's firmware by a vendor. This allows them direct control of the instrument and gives them the ability to alter the methods as they see fit. This is a burdensome process, as developing software in the firmware of a MS instrument is difficult to do and test. The programmer has to spend a lot of time understanding the firmware code that controls the MS before development on new methods can begin, often without documentation. Also, firmware modifications is notoriously difficult to test and debug, especially on the LC-time scale. With no way to debug, samples and experiments must be conducted in full to test the change of a single variable, a very slow process of programming. If there are any bugs in the code, hours could be wasted trying to detect and locate them. Improvements in how new methods are made are needed to make developing a faster and more productive process.

Distributing the instrument's firmware is not ideal way of allowing access and is more of band-aid fix then a real solution. Most instrument vendors never developed a system to support third-party methods, so when the first researchers wanted to extend the instrument capabilities, the quickest solution was to give them access to the source code, just like they were an internal developer. After they have developed their technologies, in order for other research groups to use them, the developers had to work with the vendors to commercialize their products. This process is ineffective and slow, and prohibits mass distribution. 

A better model of development would be to develop and deploy an application programming interface (API) to control the instrument. Here, the vendor would provide a set of software tools and objects to enable access to different parts of the instrument. They would have fine-grain support on what they make available to the end user and what they keep hidden, thus alleviating some IP concerns. This technique also could provide error checking, preventing the user from setting some value that could potentially damage the instrument or injure someone. In fact, some instrument vendors have started down this path. Thermo has recently released an API for their Q Exactive mass spectrometer to enable third-party support. This allows the user to program in an more advanced language then is used on the instrument itself. For example, the Q Exactive's firmware is written in Python (a scripting language), while the API is written in C\# (a compiled, and generally a more powerful language). This makes programming the instrument compatible with libraries such as CSMSL discussed in Chapter 4. An API model also allows users to share their code without IP issues, and could greatly improve the code when multiple developers are working together. 

The biggest challenge is convincing the instrument vendors to support such a technology, as it requires time and resources to develop and maintain. But if anything can be learned from community-developed applications on the internet (i.e., crowd sourcing), much can be gained when many people are working on a common problem. It may well behoove vendors to provide such access to potentially gain dozens of developers. The other issue is the distribution of software to other researchers. An ideal solution would again mimic the internet, by constructing a central marketplace to download and install methods. This would greatly facilitate the distribution aspect and improve the accessibility.

