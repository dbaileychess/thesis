\chapter{Proteomic Acquisition Strategies for Mass Spectrometry}

\section{Proteomics and Mass Spectrometry}

\subsection{Proteomics.}
Proteomics is the large-scale study of proteins, which are an essential part of life. If genes are the blueprints for life, then proteins are the construction workers, building supplies, and tools that empower and sustain life. Their involvement in life ranges from the diseases and aliments that cause impairment, to the therapeutics and medicines that cure them. From agriculture and food that provides energy, to the composition and structure of cells, there is very little in life that proteins do not affect. Understanding their role and function in biological systems is an overarching goal of life sciences. 

Proteins are structurally similar to deoxyribonucleic acid (DNA); both are long, linear chemical polymers comprised of different monomers in a sequence. DNA has four nucleotide bases (G,T,A,C) and proteins are made up of roughly twenty amino acids (A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y). The sequential order of these monomers in both DNA and proteins encodes information. The information stored in DNA is transcribed to mRNA and translated into proteins; DNA can be thought of as an instruction book, or blueprint for life. It provides the recipes to make all the proteins in the cell. On the other hand, the sequence of amino acids in proteins isn't used to store instructions, but rather it defines their structure. Proteins fold into complex three dimensional structures depending on their amino acid sequence and it is these 3D structures that provide the different mechanical and chemical functions for life to work.

A single organism can contain hundreds or thousands of different protein sequences depending on its complexity, and that set of proteins is called the proteome. The proteome of baker's yeast (\emph{Saccharomyces cerevisiae}) contains \textasciitilde4,600 different protein sequences. The human (\emph{Homo sapiens}) proteome is much larger, with nearly 12,000 unique protein sequences that are known to be expressed. The interactions between proteins and other molecules within the cell provides many of the critical functions for life to exist and procreate. Proteins are essential to most cellular processes, and when they fail to adequately perform their duty, they can cause serious problems, even death. Thus it is very important to understand organisms' proteomes and how they are affected by different treatments and conditions. Understanding how proteins change in abundance and are covalently modified by chemical signals (e.g., phosphorylation, acetylation, ubiquitination, etc.), and alter their interactions with each other are important parts of the scientific field of study called 'systems biology'. Studying large and complex systems, such as the cell, relies on the identification and quantitation of large portions of a organism's proteome. However, detecting thousands of proteins and measuring their abundances is a daunting task. Only in the last two decades have technologies been developed that are capable of identifying and analyzing such large sets of proteins, and more often than not, the technology of choice is mass spectrometry (MS). 

\subsection*{Mass Spectrometry.}
Mass spectrometry is a powerful analytical tool for measuring the mass of molecules. Current mass spectrometers can regularly measure mass to a single dalton (Da) or lower and are sensitive enough to detect as little as a few thousands molecules. Since each amino acid has a different mass, mass spectrometry is an ideal analytical technique to study and identify proteins. A mass spectrometer is an instrument that measures the mass of ions and is comprised of three parts: 1) an ionization source, 2) a mass analyzer, and 3) a detector. 

Gas phase ions (negative or positive) are first generated by an ionization source from an analyte in either the solid or liquid phases. There are many ionization techniques in use today: hard-ionization methods such as electron impact ionization (EI) causes the analyte to fragment during ionization. Softer methods that minimize fragmentation include fast-atom bombardment (FAB)\cite{fab}, matrix assisted laser desorption ionization (MALDI)\cite{maldi,maldi2}, electrospray ionization (ESI)\cite{esi}, and chemical ionization (CI)\cite{ci}, among others. ESI has become the primary method for large-scale proteomic studies because of the ease which it can be coupled to a front-end separation technique such as liquid chromatography (LC). An ESI emitter is constructed at the end of a LC column, and ionizes the eluting proteins into the MS (LC-MS). Separation is often needed in large-scale proteomic experiments because samples can consist of overly complex mixtures of thousands of analytes, and separating them prior to ionization increases sensitivity. 

The second part of a mass spectrometer is the mass analyzer, which separates the gas phase ions based on their mass-to-charge ratios (\mz{}). To effectively manipulate ions in the gas phase, various electrical devices have be developed to store, move, and analyze them. Since ions are charged particles, only their \mz{} ratio is detected, and their mass (\emph{m}) is not directly measured. However, the mass can be calculated from the \mz{} if the number of charges is known. There are many different types of mass analyzers: magnetic sector, time-of-flight (TOF)\cite{tof}, quadrupole mass filters\cite{iontrap}, ion-traps\cite{iontrap2}, Orbitraps\cite{orbitrap}, and fourier transform ion cyclotron resonsnace (FT-ICR)\cite{fticr}. Each analyzer relies on a different principle to separate ions by \mz{}, and some are able to separate better and/or faster than others. The Orbitrap and FT-ICR are capable of separating closely spaced \mz{} ions (i.e., high resolution), while ion-traps and TOFs can mass analyze very quickly with sufficient resolution. Each analyzer has its pros and cons, so it has become common to include multiple mass analyzers in one instrument to increase the capabilities of the MS. These multi-analyzer instruments are called hybrid instruments as they blend multiple techniques into one package.

The final portion of a mass spectrometer is the detector. Following mass separation, the ions need to be detected and converted into electrical signals to be recorded. Some detectors are destructive, consuming the ion when detected. Examples of these include Faraday cups and electron multipliers. Some detectors are non-destructive, being able to detect the ions without consuming them. These detectors are unique in that they can act both as a mass analyzer and detector. The Orbitrap and FT-ICR are examples of non-destructive, inductive detectors where an AC current is produced by ions oscillating within the detector. This generates an AC current in the detector that is stored and subsequently Fourier transformed into a \mz{} spectrum. These inductive detectors rely on Fourier transformation which scales with the length of acquisition. Thus, the longer the ions are detected the higher the resolution and increased signal-to-noise (S/N) achieved.

Mass spectrometers are powerful instruments that ionize, separate, and detect various analytes. While determining the mass of analytes is useful in and of itself, mass analysis of proteins is more convoluted. This is because the order of amino acids in a protein does not change the total mass. Two functionally different proteins could have the same amino acid contents, but in a different order, and would appear at the same \mz{} value. To ascertain their sequence---and determine their identity, additional analysis steps are needed. First, a set of ions are injected into the MS and a full scan is taken (MS or MS1). Then a certain \mz{} feature is isolated from the other ions in the MS. These isolated ions are then dissociated into smaller pieces (fragments) and mass analyzed again (MS/MS). Proteins can be dissociated by a variety of different methods. The most common approach is to forcefully collide the ions with background gas molecules that are present in the instrument (Collision-Activated Dissociation, CAD). Other approaches exist, such as electron-capture dissociation (ECD)\cite{ecd} and electron-transfer dissociation (ETD)\cite{etd}, as well as infrared mulitple photon dissociation (IRMPD)\cite{irmpd} and higher energy C-trap dissociation (HCD)\cite{hcd}. Following dissociation, a fragmentation spectrum is collected and can help provide clues on the sequence of the protein being analyzed. This process is called tandem mass spectrometry, as multiple mass anlaysis steps are taken to identify proteins. While tandem mass analysis of intact proteins is possible, their large size, complex fragmentation spectra, and poor separability make large-scale analysis of proteins challenging. One popular approach called bottom-up proteomics alleviates some of these issues by breaking proteins apart into smaller pieces and mass analyzing those.

\section{Acquisition Methods for Shotgun Proteomics}
Shotgun proteomics is the process of digesting proteins into smaller pieces, called peptides, prior to separation and mass analysis (LC-MS).  This scheme (bottom-up proteomics) offers many benefits over mass analysis of intact proteins (top-down proteomics). First, the smaller size of peptides makes separation by LC simpler. Since peptides have less 3D structure than proteins, they interact more with the stationary phase of the separation, which helps improves the separation. Differences in ionization between peptides and proteins is another major factor. Peptides often ionize into a smaller number of charge states than proteins because they contain fewer charge-carrying sites. This concentrates the signal into fewer states, increasing the S/N for any given one. Proteins often exist in dozens of different charge states, which dilutes the S/N among them. On top of the charge state distribution, the wide isotopic distributions of proteins further decreases the S/N and increases spectral complexity. The distribution of \mz{} for peptides is also centered in the optimal mass range of most mass spectrometers (e.g., 300 - 1500 \mz{}). Finally, the smaller the analyte the less complex the fragmentation spectra are and that usually means easier interpretation and identification. These and other reasons make peptide mass analysis easier than intact protein mass analysis.

While bottom-up proteomics offers many advantages compared to top-down analysis, new challenges also arise. First, the sample becomes much more complex. For example, the proteome of yeast contains \textasciitilde4,600 proteins, but when they are digested into peptides by proteases, such as trypsin, hundreds of thousands of peptides result. Another issue that surfaces is increased ambiguity in protein identification. Following sequencing of peptides by LC-MS/MS, computer algorithms map these peptides back to their parent proteins. But shorter peptide sequences are more likely to have originated from more than one protein, obscuring which protein was actually identified. This can be partially combated by identifying other peptides from those proteins to help distinguish them apart. Other challenges are also present, but most proteomic publications make use of the shotgun proteomic scheme.

The typical shotgun proteomic workflow begins with protein extraction from cell cultures or tissues. Proteins are then isolated and digested into peptides by proteolysis and are separated by liquid chromatography. Peptides elute from the LC column and are ionized by ESI into the MS. From here a variety of acquisition methods are can be used to generate tandem mass spectra (MS/MS) of the eluting peptides; these will be discussed in detail below. Following acquisition, the collection of spectra are searched against a database of protein sequences to identify the peptide sequences from the fragmentation spectra. This is followed by statistical analysis that culls out false identifications.\cite{targetdecoy} Finally, the identified peptides are assembled back into protein groups and optional quantitative analysis can be conducted. This results in a list of identified proteins, their relative abundances, and possible post-translational modifications (PTMs). This workflow first emerged in 2001 and has changed very little since.\cite{mudpit} Improvements to each part of this worklow, especially the advent of new hybrid mass spectrometers, have propelled this methodology from identifying 1,483 yeast proteins in \textasciitilde68 hours to >4,000 proteins in an hour, all in the past decade.\cite{onehour}

Probably the one aspect of this workflow that has changed the least is the data acquisition method---how the MS decides what \mz{} to dissociate and MS/MS analyze. The two overarching methods for data acquisition in a shotgun experiment are the data-dependent and data-independent methods. Data-dependent acquisition (DDA) relies on surveying all the intact \mz{} of peptide precursors with a full MS scan, followed by successive isolation and fragmentation of different \mz{} peaks based on their intensity.\cite{dda1,dda2} Data-independent acquisition (DIA), on the other hand, forgoes the initial survey scan and iteratively isolates and fragments different \mz{} regions in a predictable fashion.\cite{dia1,dia2}

\subsection*{Data-Dependent Acquisition}
Data-dependent acquisition relies on gathering information about the peptides currently eluting to select the best candidates for MS/MS sampling. It is typically performed with an initial MS scan to mass analyze all the peptide precursors. Then the mass spectrometer selects the top $N$ most intense \mz{} features to undergo dissociation and MS/MS analysis in subsequent scans. The value of $N$ is typically between 5 and 20 depending on the instrument. Following acquisition of $N$ MS/MS spectra, the whole process is repeated with another survey scan. This straightforward and simple method is highly effective and has been relatively unchanged since its debut.

The DDA method has been supplemented with additional filtering criteria to improve the diversity of \mz{} peaks sampled. With the dynamic exclusion filter, closely-spaced \mz{} features are excluded from being reselected within some time range since it was first selected (e.g., 30 seconds). This prevents the repeated sampling and identification of the same precursor and tries to sample a wider population of precursors. Other DDA filtering criteria try to avoid sampling precursors that will not produce identifiable fragmentation spectra. With high-resolution spectra, \mz{} features that are singly charged are often avoided as they produce poor MS/MS spectra. Other filters look at the isotopic ratios of the analytes and avoid sampling features that don't display peptidic ratios. These and other filters help DDA segment the MS time to select a diverse set of peptide precursors during an experiment with high throughput.

One of the biggest issues with DDA is that the analyte must be detected in the initial survey scan in order to be sampled and identified. If a precursor, for whatever reason, never exceeds a S/N threshold to be selected, it will never be identified. This makes DDA a stochastic process, depending on the quality of the survey scan data to make its future decisions. This leads to irreproducible sampling and identification across multiple experiments, leaving datasets incomplete. The other acquisition method, Data-independent acquisition, tries to avoid this issue by skipping the survey scan altogether.

\subsection*{Data-Independent Acquisition}
Data-independent acquisition is a methodology where the MS iteratively isolates and dissociates different \mz{} regions regardless of detection in a survey scan. There are a few different approaches that DIA methods can take. The simplest is to repetitively isolate and dissociate the same \mz{} region for the entire LC separation. This guarantees that any peptide precursor whose \mz{} is within the isolation range will be analyzed. This method is extremely low-throughput though, only able to analyze a few dozen different precursors over the course of the separation.

Increased throughput can be gained by changing the isolated \mz{} region throughout the separation. For example, in the first MS/MS scan the \mz{} range 300 - 303 is isolated and mass analyzed. The following scan the \mz{} range changes to 303 - 306, the third scan to 306 - 309, and so forth. After the complete mass range is analyzed once (e.g., 300-1200 \mz{}), it starts over at 300 - 303 \mz{}. This approach samples all possible \mz{} ranges during the experiment, but each individual \mz{} range must wait until all the other ones have been sampled. This leads to long times between analysis and could lead to completely missing an eluting analyte from being sampled. However, when not limited by time or sample amounts this approach can produce a comparable number of identifications to DDA methods.\cite{panda} 

Probably the most popular DIA method is selected reaction monitoring (SRM).\cite{mrm2,mrm3} This type of experiment is usually conducted on a triple-quadrupole instrument in a scheduled fashion. Scheduling involves breaking up the LC-MS/MS experiment into different time segments, and targeting a subset of peptides per segment. In an SRM experiment, the first quadrupole isolates a single precursor during its expected elution time, the second quadrupole fragments those ions, and the final quadrupole isolates another \mz{} before the ions reach the detector. SRM is a very sensitive and selective method, and is the gold-standard for targeted proteomics. The method, however, is low-throughput and is able to target only a few hundred peptides per LC-MS experiment.\cite{} SRM methods also do not usually include a full MS/MS spectrum for each peptide, favoring increased sensitivity at the cost of more ambiguity. Recent work by our lab and others have extended the SRM method to work on high-resolution instruments with a full MS/MS scan being acquired, a method called parallel reaction monitoring (PRM).\cite{prm1,prm2} 

More recent advances in DIA methods include isolating and mass analyzing large \mz{} regions. Here, instead of 3 \mz{} isolation ranges, the isolation window is open up to 10, 20 or even 50 \mz{}. Methods such as ''sequential windowed data independent acquisition of the total high-resolution mass spectra'' (SWATH) and MS everything (MS$^E$) seek to further improve the throughput and reproducibility of DIA methods with informatic advances.\cite{swath,mse} Here multiple precursors are co-isolated and co-fragmented, producing complex fragmentation spectra that are deconvoluted post-acquisition. This increases throughput as less \mz{} regions need to be analyze per cycle. However, post-translational modification (PTM) analysis is stymied because the complex fragmentation spectra which are produced make PTM localization a very difficult process.

\section{Intelligent Data Acquisition}
Although most mass spectrometry-related technologies have constantly improved over the past two decades, improvements to the data acquisition methods have progressed at a far slower pace. The DDA strategy has only been modestly supplemented in the past decade while the DIA strategy seeks to remove all intelligence from the acquisition in favor of simplicity. Perhaps the biggest step forward in increasing the performance of DDA methods was the introduction of the ETD-CAD decision tree (DT) algorithm.\cite{decisiontree} Here, precursors were either dissociated with ETD or CAD based on their \mz{} and charge-state, increasing identification rates. Unlike DDA, the DT algorithm incorporated multiple pieces of data (\mz{}, \emph{z}, intensity, etc.) to make an advanced decision.

Following the advent of the DT algorithm, which relied on advanced analysis of a MS scan, we thought it could be expanded to the analysis of MS/MS scans. Direct analysis of MS/MS scans in real-time provides a lot more information that could be used to make more advanced decisions. The following chapters describe some of the first work on improving the intelligence of MS acquisition methods for proteomic research. These methods, grouped under the term intelligent data acquisition (IDA), represent new methodologies in how the MS selects peptide precursors and improves the data quality. In chapter 2, the development of the first real-time database searching algorithm (\inseq{}) is described. The MS was empowered to identify MS/MS spectra immediately following acquisition, allowing for the MS to decide what to do next. Improvements to reproducibility, quantitative accuracy, and PTM analysis are demonstrated over traditional DDA methods. Chapter 3 summarizes our real-time algorithms for improving the run-to-run reproducibility of peptide identification. Peptides are scheduled based on their relative elution order instead of the more typical absolute retention times. The IDA method could determine the overall elution order by analyzing survey MS scans, and then subsequently target peptides in a DIA fashion. The method increased the number of peptides identified in repeated experiments by over 50\%. Chapter 4 is devoted to the programming and software frameworks used for data analysis and real-time control. The chapter starts off with a summary of the Coon OMSSA Proteomic Analysis Software Suite (COMPASS) and the improvements made to it since its initial publication.\cite{compass} The suite is a complete data-analysis package for tandem mass spectrometry data. The chapter continues with a brief discussion on the developments of the C\# Mass Spectrometry Library (CSMSL). This library provides many tools and programs to develop new analysis programs in a fast and easy manner. The final chapter in this document looks at the future of IDA methods and what challenges they face and offers a few suggestions on possible solutions.

\bibliographystyle{ieeetr}
\bibliography{intro}