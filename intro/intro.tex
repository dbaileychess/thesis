\chapter{Proteomic Acquisition Strategies for Mass Spectrometry}

\section{Proteomics and Mass Spectrometry}

\subsection{Proteomics.}
Proteomics is the large-scale study of proteins. Proteins are an essential part of life and every living thing contains proteins. If genes are the blueprints for life, then proteins are the construction workers, building supplies, and tools that empower and sustain life. Their involvement in life ranges from the diseases and aliments that cause impairment to the therapeutics and medicines that cure them. From agriculture and food that provides energy to motion and structure that defines the shape of cells. There is very little in life that proteins do not affected. Understanding their role and function in biological systems is an important goal of life sciences. 

When compared to deoxyribonucleic acid (DNA), proteins are very similar. Both are long, linear chemical polymers comprised of different monomers: DNA has 4 nucleotide bases (G,T,A,C) and proteins are made up of roughly 20 amino acids (A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y). The sequential order of these monomers in both DNA and proteins encodes information. The information stored in DNA is primarily used to construct proteins and can be thought of as an instruction book, or blueprint. On the other hand, protein sequences isn't used to store instructions, but rather their structure. Proteins fold into complex three dimensional structures depending on their amino acid sequence and it is these 3D structures that provide the different mechanical and chemical functions for life to work.

A single organism will often contain hundreds or thousands of different protein sequences---depending on their complexity, and that set of proteins is called the proteome. For example, the proteome of baker's yeast (\emph{Saccharomyces cerevisiae}) contains \textasciitilde4,600 different protein sequences. The human (\emph{Homo sapiens}) proteome is much larger, with nearly 12,000 different protein sequences known to be expressed. Often in biology it is important to know how an organism's proteins change when exposed to different conditions. To detect thousands of proteins is a daunting task, and it has only been in the last two decades have technologies been capable of identifying large portions of proteomes. 

The main technology used is mass spectrometry (MS)

\subsection*{Mass Spectrometry.}
Mass spectrometry is a powerful analytical tool. A mass spectrometer is an instrument that measures the mass of ions and is comprised of three parts: 1) an ionization source, 2) a mass analyzer, and 3) a detector. 

Gas phase ions (negative or positive) are first generated from an analyte in either the solid or liquid phases by the ionization source. There are many ionization techniques in use today, hard-ionization methods such as electron impact ionization (EI) causes the molecule to fragment during ionization. Softer methods that minimize fragmentation include fast-atom bombardment (FAB), matrix assisted laser desorption ionization (MALDI), electrospray ionization (ESI), and chemical ionization (CI), among others. ESI has become the primary method for large-scale proteomic studies because of the ease it which it can couple to a front-end separation methods such as liquid chromatography (LC). Separation is needed in most proteomic experiments because the samples consists of thousands of peptides, and separating them prior to ionization is an important factor in increasing sensitivity. 

The second part of the mass spectrometer is the mass analyzer, which separates the gas phase ions based on their mass-to-charge ratios (\mz{}). There are many different types of mass analyzers: magnetic sector, time-of-flight (TOF), quadrupole mass filters, ion-traps, orbitraps, and fourier transform ion cyclotron resonsnace (FT-ICR). Each rely on a different principle to separate ions by their \mz{}, and some are able to separate better or faster than others. The orbitrap and FT-ICR are capable of separately closely spaced \mz{} ions (high resolution), while ion-traps and TOFs are can mass analyzed very quickly. Each analyzer has it pros and cons, and it is typical to have multiple types of analyzers in one instrument to increase the flexibility of the experiments.

The final portion of a mass spectrometer is the detector. Following mass separation, the ions need to be detected and converted to electrical signals to be recorded. Some detectors are destructive, consuming the ion when detected. Examples of these include Faraday cups and electron multipliers. Fourier transform detectors are non-destructive, being able to detect the ions without consuming them. These detectors are unique in that they act both as a mass analyzer and detector in one. The obritrap and FT-ICR are examples of inductive detectors where an AC current is produced by ions oscillating within the detector. This AC current is subsequently Fourier transformed into a \mz{} spectrum by a microchip.

Mass spectrometers are powerful instruments that ionizes, separates, and detects the \mz{} of various analytes. While determining the mass of analytes is useful in and of itself, mass analysis of proteins is more convoluted. This is because the sequence of amino acids is hidden, i.e., there are many different orderings of amino acids that would give you the same total mass. To ascertain their sequence---and determine their identity by mass spectrometry, additional steps are needed. Protein ions are first isolated by the MS to remove all other ions from the instrument. These isolated ions are then dissociated into smaller pieces (fragments) by a variety of different methods, and then mass analyzed again (MS/MS) and the spectrum is recorded. This fragmentation spectra gives clues on the sequence of the protein being analyzed. This process is called tandem mass spectrometry, as multiple mass anlaysis steps are taken to help identify proteins. While tandem mass analysis of protiens is possible, their large size, complex fragmentation spectra, and poor separability make large-scale analysis of proteins challenging. One popular approach to alleviate these issues is to break the proteins down into smaller pieces, a process called shotgun proteomics.

\section{Acquisition Methods for Shotgun Proteomics}
Shotgun proteomics is the process of digesting proteins into smaller pieces, called peptides, prior to separation and mass analysis (LC-MS).  This scheme (bottom-up proteomics) offers many benefits over mass analysis of intact protein (top-down proteomics). First, the smaller size of peptides makes separation by LC simpler. Peptides have less 3D structure then proteins, which aids in the interaction with the stationary phase of the separation and improves the separation. Differences in ionization between peptides and proteins is another major factor. Peptides often ionize into fewer number of charge states than proteins due to fewer charge-carrying sites. This increases the signal-to-noise (S/N) for any charge state, while proteins often exist in dozens of different charge states, each of which dilutes the S/N. On top of the charge state distribution, wide isotopic distributions for proteins further decrease the S/N compared to peptides. The distribution of \mz{} of peptides is also more centered in the optimal mass range of most mass spectrometers. Finally, the smaller the analyte the less complex the fragmentation spectra are and that usually means easier interpretation and identification. 

While bottom-up proteomics offers many advantages compared to top-down anlaysis, new challenges also arise. First, the sample becomes much more complex. For example, the proteome of yeast is \textasciitilde4,600 proteins in size, but when they are digested into peptides by protease such as trypsin, results in hundred of thousands of peptides. Another issue that arises is increased ambiguity in protein identification. Following sequencing of peptides by LC-MS, computer algorithms are used to map these peptides back to their parent proteins. But shorter peptide sequences are more likely to have originated from more than one protein, obscuring which protein was actually identified. Other challenges are also present, but most proteomic publications make use of the shotgun proteomic scheme.

The typical shotgun proteomic workflow begins with protein extraction and isolation from cell cultures or tissue samples. Proteins are then digested into peptides by proteolysis and are separated by liquid chromatography. Peptides elute from the LC column and are ionized by an ESI directly into the MS. From this step on, a variety of acquisition methods are used to generate tandem mass spectra (MS/MS) of all the eluting peptides. Following acquisition, the collection of spectra are searched with algorithms to identify the peptide sequences from the fragmentation spectra and further statistical analysis is performed to weed out false identifications. Finally, identified peptides are assembled back into protein groups and optional quantitative analysis can be conducted. This workflow first emerged in 2001 and has changed very little since.\cite{mudpit} Improvements to each part of this worklow, especially the advent of new hybrid mass spectrometers, have propelled this methodology from identifying 1,483 yeast proteins in \textasciitilde68 hours to >4,000 proteins in an hour, all in the past decade.\cite{onehour}

Probably the one aspect of this workflow that has changed the least is how the MS decides what \mz{} to fragment and MS/MS analyze. The two overarching methods for data acquisition in a shotgun experiments are the data-dependent and data-independent methods. Data-dependent acquisition (DDA) relies on surveying all the intact \mz{} of peptide precursors with a full MS scan, following by successive isolation and fragmentation of different \mz{} peaks based on their intensity.\cite{dda1,dda2} Data-independent acquisition (DIA), on the other hand, forgoes the initial survey scan and iteratively isolation and fragments different \mz{} regions in a predictable fashion.\cite{dia1,dia2}

\subsection*{Data-Dependent Acquisition}
Data-dependent acquisition relies on gathering information about the currently eluting peptides to select the best candidates for MS/MS sampling, it depends on the data. It is typically perform with an initial MS scan to mass analyze all the peptide precursors currently eluting from the LC column. The mass spectrometer then selects the top $N$ most intense \mz{} features to undergo dissociation and MS/MS analysis in subsequent scans. The value of $N$ is typically between 5 and 20 depending on the instrument. Following acquisition of $N$ MS/MS spectra, the process is repeated with another survey scan. This straightforward and simple method is highly effective and has been relatively unchanged since its debut.

The DDA method has been supplemented with additional filtering criteria to improve the diversity of \mz{} peaks that are sampled. With the dynamic exclusion filter, closely-spaced \mz{} features are excluded from being reselected within some time range since it was first selected (e.g., 30 seconds). This prevents the repeated sampling and identification of the same precursor and tries to sample a wider population of precursors. Other DDA filtering criteria try to avoid sampling precursors that will not produce identifiable fragmentation spectra. With high-resolution spectra, \mz{} features that are singly charged are often avoided as their produce poor MS/MS spectra. Other filters look at the isotopic ratios of the analytes and avoid sampling features that don't display peptidic ratios. These and other filters help DDA segment the MS time to select a diverse set of peptide precursors during an experiment.

One of the biggest issues with DDA is that the analyte must be detected in the initial survey scan in order to be sampled and identified. If a precursor, for whatever reason, never exceeds a S/N threshold to be selected, it will never be identified. This issue is not the end of the world, as there are often many peptides per protein that can be identified if a particular one doesn't ionize well. DDA is a stoachastic process, depending on the quality of the survey scan data to make its decisions. This can lead to irreproducible sampling and identification across multiple runs, resulting in incomplete datasets. Data-independent acquisition tries to avoid this issue by being independent of data in a survey scan.

\subsection*{Data-Independent Acquisition}
Data-independent acquisition is a methodology where the MS iteratively isolation and dissociates different \mz{} regions regardless of detection in a survey scan or prior knowledge. There are a few different ways DIA methods can work. The simplest is to repetitively isolate and dissociate the same \mz{} region for the entire LC separation. This guarantees that any peptide precursor whose \mz{} is within the isolation range will be analyzed. This method is extremely low-throughput though, only able to analyze a few dozen different precursors over the course of the separation.

Increased throughput can be gained by changing the isolated \mz{} region throughout the separation. For example, in the first scan the \mz{} range 300 - 303 is isolated and mass analyzed. The second scan the range changes to 303 - 306, the third scan 306 - 309, and so forth. After the complete mass range is analyzed once, it starts over again at 300 - 303 \mz{}. This approach has seen comparable number of identification to DDA methods when not limited by time or sample amounts.\cite{panda} 

Probably the most popular DIA method is selected reaction monitoring (SRM).\cite{mrm1,mrm2,mrm3} This type of experiment is usually conducted on a triple-quadrupole instrument in a scheduled fashion. The first quadrupole isolates a single precursor during its expected elution time, the second quadrupole fragments the ions, and the final quadrupole isolates one fragment ion before they reach a detector. SRM is very sensitive and selective method, and is the gold-standard for targeted proteomics. The method is, however, low-throughput and is able to target a only a few hundred peptides per LC-MS experiment. Recent work by our lab and others have extended the SRM method to work on high-resolution instruments with a full MS/MS scan being acquired, a method called parallel reaction monitoring (PRM).\cite{prm1,prm2} 

More recent advances in DIA methods include isolating and mass analyzing larger \mz{} regions. Here, instead of isolating a 3 \mz{} region, the isolation window is open up to 10, 20 or even 50 \mz{}. Methods such as ''sequential windowed data independent acquisition of the total high-resolution mass spectra'' (SWATH) and MS everything (MS$^E$) seek to further improve the throughput and reproducibility of DIA methods with informatic advances.\cite{swath,mse} Here multiple precursors are co-isolated and co-fragmented, producing complex fragmentation spectra that are deconvoluted post-acquisition. This increases throughput as less \mz{} regions need to be analyze per cycle. However, post-translational modification (PTM) analysis is stymied because of the complex fragmentation spectra produce makes PTM localization a very difficult process.

\section{Advanced Acquisition Techniques}
Both DDA and DIA methods have their advantages and shortcomings. 

The following chapters describe some of the first work on improving the intelligence of the MS acquisition methods for proteomic research. These methods, grouped under the term intelligent data acquisition (IDA), represent new methodologies in how the MS selects peptide precursors. In chapter 2, the development of the first real-time database searching algorithm (\inseq{}) is described. The MS was empowered to identify MS/MS spectra immediately following acquisition, allowing for the MS to decide what to do next. Improvements to reproducibility, quantitative accuracy, and PTM analysis are demonstrated over traditional DDA methods. Chapter 3 summarizes our real-time algorithms for improving the run-to-run reproducibility of peptide identification. Peptides are scheduled based on their relative elution order instead of the more typical absolute retention times. The IDA method could determine the overall elution order by analyzing survey MS scans, and then subsequently target peptides in a DIA fashion. The method increased the number of peptides identified in repeated experiments by over 50\%. Chapter 4 is devoted to the programming and software frameworks used for data analysis and real-time control. The chapter starts off with a summary of the Coon OMSSA Proteomic Analysis Software Suite (COMPASS) and the improvements made to it since its initial pulication.\cite{compass} The suite is a complete data-analysis package for analysis tandem mass spectrometry data. The chapter continues with a brief discussion on the developments of the C\# Mass Spectrometry Library (CSMSL). This library provides many tools and programs to develop new analysis programs in a fast and easy manner. The final chapter in this document looks at the future of IDA methods and what challenges they face and offers a few suggestions on possible solutions.

\bibliographystyle{ieeetr}
\bibliography{intro}

%history
%DDA/DIA
%Dynamic Exclusion
%Decision Tree
%inseq
%eoa
%future
